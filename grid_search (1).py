# -*- coding: utf-8 -*-
"""grid_search.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1x4t9oBsO-_G93ISZLl9OhvvP6hQunTdC
"""

import pandas as pd
import numpy as np
import itertools
import csv  

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import StratifiedShuffleSplit

from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.linear_model import RidgeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import Lasso

from sklearn.metrics import homogeneity_score, make_scorer
import warnings
warnings.filterwarnings("ignore")


matches = pd.read_csv('matches.csv', header=None, names=['match_id','fid','home_team_id','away_team_id','date','league_id','season',
                                                         'home_team_goals','away_team_goals','home_team_name','away_team_name','home_team_xg','away_team_xg',
                                                         'home_team_win_chance','drow_chance','away_team_win_chance','league_name','home_team_shots','away_team_shots',
                                                         'home_team_shots_on_target','away_team_shots_on_target','home_team_deep','away_team_deep','away_team_ppda','home_team_ppda'])
matches = matches.sort_values(by='date' ,ignore_index=True)
data_feature_en = pd.read_csv('data_feature_en.csv')
data_feature_en = data_feature_en[matches['league_id'] != 6]
def get_result(scored, conceded):
  if scored > conceded:
    return 'H'
  elif scored <= conceded:
    return 'noH'

data_feature_en['result'] = data_feature_en.apply(lambda x: get_result(x[3], x[4]), axis=1)

data_feature_en['coeff_noH'] = (data_feature_en['coeffA']*data_feature_en['coeffD'])/(data_feature_en['coeffA']+data_feature_en['coeffD'])
data_without_nones = data_feature_en.replace(to_replace='None', value=np.nan).dropna()

def quality(y_test, prediction):
  #print(prediction)
  #print(y_test)
  X_test = data_without_nones.loc[y_test.index]
  #print(X_test)
  #print(X_test[prediction == y_test])
  
  exact_data = X_test[prediction == y_test]
  sum = 0
  count_D= 0
  for item in exact_data.index:
    if y_test[item] == 'H':
      sum += X_test.loc[item,:]['coeffH']
    elif y_test[item] == 'noH':
      count_D += 1
      sum += X_test.loc[item,:]['coeff_noH']  
  return sum/len(X_test) - 1

def normalize(X):
  normalized_X = pd.DataFrame(preprocessing.normalize(X), index=X.index, columns=X.columns)
  return normalized_X

def add_model_to_file(accuracy, best_quality, train_quality, estimator, best_params):
    fields=[accuracy, best_quality, train_quality, estimator, best_params]
    with open('models,csv', 'a') as f:
        writer = csv.writer(f)
        writer.writerow(fields)

models = [(MultinomialNB(), {'alpha': np.linspace(0.00001, 1, 500)}),
          (KNeighborsClassifier(), {'n_neighbors': np.arange(1,100)}),
          (MLPClassifier(), {'solver': ['lbfgs'], 'max_iter': np.arange(1000, 10000, 500), 'alpha': np.linspace(0.00001, 1, 500), 'hidden_layer_sizes':np.arange(1, 50)}),
          (RandomForestClassifier(), { 'max_depth': range(7, 30), 'n_estimators': np.arange(100, 1000, 10)}),
          (LogisticRegression( class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=100,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), {'C': np.linspace(0.0001, 1, 150)}),
          (GradientBoostingClassifier(), {"loss":["deviance"],"learning_rate": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],
                                                        "min_samples_split": np.linspace(0.1, 0.5, 1, 2),"min_samples_leaf": np.linspace(0.1, 0.5, 1, 2),
                                                        "max_depth":[3,5,8],"max_features":["log2","sqrt"],"criterion": ["friedman_mse",  "mae"],
                                                        "subsample":[0.5, 0.618, 0.8, 0.85, 0.9, 0.95, 1.0],"n_estimators":[10, 50, 70, 100, 130]}),
          (SVC(), {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']})
          ]




scorer = make_scorer(quality, greater_is_better=True)
info = []
X = data_without_nones[['mean_away_goals_scored_last_6', 'mean_home_deep', 'mean_away_shots_conceded_last_6', 'mean_home_win_chance_last_6', 'mean_home_ppda_last_6', 'mean_away_goals_conceded_last_6', 'mean_home_shots_scored', 'mean_away_conceded_xg_last_6', 'mean_away_conceded_xg_last_6', 'mean_home_ppda', 'mean_away_drow_chance', 'mean_away_drow_chance', 'mean_home_win_chance', 'mean_home_drow_chance_with_this_away', 'mean_home_drow_chance', 'mean_away_drow_chance', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_home_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_home_drow_chance_with_this_away', 'mean_away_conceded_xg_last_6', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_home_drow_chance_with_this_away', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance', 'mean_away_drow_chance', 'mean_home_drow_chance_with_this_away', 'mean_home_drow_chance_with_this_away', 'mean_home_drow_chance_with_this_away', 'mean_away_drow_chance']]
y = data_without_nones['result']

normalized_X = normalize(X)
for_test = 1000
X_train = normalized_X[:-for_test]
X_test = normalized_X[-for_test:]
y_train = y[:-for_test]
y_test = y[-for_test:]

for model in models:
  estimator = model[0]
  parameters = model[1]
  clf = GridSearchCV(estimator, parameters,scoring=scorer,  n_jobs=-1,
                                                        cv = StratifiedShuffleSplit(n_splits=5,test_size=0.25,random_state = 0))
  clf.fit(X_train, y_train)
  train_prediction = clf.predict(X_train)
  prediction = clf.predict(X_test)
  add_model_to_file(accuracy_score(prediction, y_test), quality(y_test,prediction), quality(y_train, train_prediction), estimator, clf.best_params_)
  info.append((clf, accuracy_score(prediction, y_test), quality(y_test,prediction)))
  print(clf.best_params_, accuracy_score(prediction, y_test), quality(y_test,prediction), quality(y_train, train_prediction))